# MCP Foundry ML Integration

> **Bridging Microsoft Foundry Agents, Model Context Protocol (MCP), and Azure Machine Learning**

A production-ready implementation demonstrating how AI agents can safely trigger enterprise machine learning workflows through standardized tool-calling mechanisms.

## ğŸ¯ Project Objective

Enable Microsoft Foundry Agents to interact with Azure Machine Learning through a secure MCP (Model Context Protocol) server, creating a **3-layer intelligent automation stack** for enterprise ML operations.

## ğŸ—ï¸ Architecture

This project implements a **3-layer intelligent automation stack**:

### Architecture Diagram

![MCP Foundry Azure ML Integration Architecture](docs/diagrams/mcp-foundry-azureml-integration.jpeg)

### Layer 1 â€” Intelligence (Foundry Agent)
- Understands user intent
- Chooses the correct tool
- Processes tool outputs
- Responds naturally in conversation

### Layer 2 â€” Integration (MCP Server)
- The "middleware conductor"
- Exposes Python functions as safe MCP tools
- Routes calls from agents to enterprise systems
- Enforces tool boundaries and validation

### Layer 3 â€” Execution (Azure ML)
- Handles real workloads (pipelines, jobs, training, experiments)
- Provides compute resources and scaling
- Ensures governance and tracking

### Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   Foundry Agent (Layer 1)                                      â”‚
â”‚   â”œâ”€ Interprets user requests                                  â”‚
â”‚   â”œâ”€ Calls MCP tools                                           â”‚
â”‚   â””â”€ Processes responses                                       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚   MCP Server (Layer 2)                                         â”‚
â”‚   â”œâ”€ greet(name)                                               â”‚
â”‚   â”œâ”€ add_numbers(a, b)                                         â”‚
â”‚   â”œâ”€ run_aml_pipeline(payload)                                 â”‚
â”‚   â”œâ”€ list_aml_experiments()                                    â”‚
â”‚   â””â”€ get_aml_job_status(job_name)                             â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚   Azure ML (Layer 3)                                           â”‚
â”‚   â”œâ”€ Executes pipelines                                        â”‚
â”‚   â”œâ”€ Manages experiments                                       â”‚
â”‚   â”œâ”€ Tracks jobs                                               â”‚
â”‚   â””â”€ Provides compute resources                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Project Structure

```
mcp-foundry-ml/
â”œâ”€â”€ server.py                      # MCP Server entry point (FastAPI)
â”œâ”€â”€ mcp_foundry_agent.py          # Foundry Agent entry point (Interactive demo)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ test-mcp-tools.http           # REST Client tests for all MCP tools
â”œâ”€â”€ .env                          # Environment variables (credentials) - NOT in git
â”œâ”€â”€ .env.example                  # Template for .env file
â”œâ”€â”€ README.md                     # This file
â”‚
â”œâ”€â”€ aml/                          # Azure ML definitions
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ pipeline.yml          # Demo pipeline definition
â”‚   â””â”€â”€ models/                   # (Reserved for model registrations)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp_server/               # Layer 2: MCP Server
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py               # FastAPI server with tool routing
â”‚   â”‚   â”œâ”€â”€ config.py             # Server configuration
â”‚   â”‚   â””â”€â”€ tools/                # MCP tools
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ utility_tools.py  # Demo tools (greet, add_numbers)
â”‚   â”‚       â””â”€â”€ azure_ml_tools.py # Azure ML tools (run_aml_pipeline, etc.)
â”‚   â”‚
â”‚   â””â”€â”€ foundry_agent/            # Layer 1: Foundry Agent
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ client.py             # Interactive agent demo with tool calling
â”‚       â””â”€â”€ bridge.py             # Azure ML operations bridge
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh                  # Automated project setup
â”‚   â””â”€â”€ create-compute.sh         # Create Azure ML compute cluster
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ IMPLEMENTATION_GUIDE.md   # Detailed implementation walkthrough
    â”œâ”€â”€ PRESENTATION_GUIDE.md     # Quick reference for tech talk
    â”œâ”€â”€ CHECKLIST.md              # Project completion checklist
    â””â”€â”€ diagrams/                 # Architecture diagrams
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.9+** (tested with Python 3.13)
- **Azure subscription** with:
  - Azure AI Foundry project
  - Azure Machine Learning workspace
  - Compute cluster (created automatically by setup script)
- **Azure CLI** installed and authenticated

### 1. Clone and Setup

```bash
# Navigate to project directory
cd mcp-foundry-ml

# Run automated setup script
chmod +x scripts/setup.sh
./scripts/setup.sh
```

The setup script will:
- âœ… Check Python version
- âœ… Create virtual environment
- âœ… Install dependencies
- âœ… Verify Azure CLI authentication
- âœ… Create `.env` file template
- âœ… Guide you through configuration

### 2. Configure Environment

Copy `.env.example` to `.env` and fill in your Azure credentials:

```bash
cp .env.example .env
# Edit .env with your actual values
```

**Required environment variables:**

```bash
# Azure ML Configuration
AZURE_SUBSCRIPTION_ID=your-subscription-id
AZURE_RESOURCE_GROUP=your-resource-group
AZURE_ML_WORKSPACE=your-workspace-name

# Microsoft Foundry Agent Configuration
PROJECT_ENDPOINT=https://your-foundry-resource.services.ai.azure.com/projects/your-project
MODEL_DEPLOYMENT_NAME=your-model-deployment
AGENT_ID=your-agent-id
AGENT_NAME=your-agent-name

# API Key (if using key-based authentication)
AZURE_AI_PROJECT_API_KEY=your-api-key

# MCP Server Configuration
MCP_SERVER_URL=http://localhost:8000/mcp/call
MCP_SERVER_NAME=MCP Foundry Bridge Server
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000
```

### 3. Create Azure ML Compute Cluster

```bash
# Automated script creates compute cluster for you
chmod +x scripts/create-compute.sh
./scripts/create-compute.sh
```

This creates a `mcp-compute` cluster (Standard_DS3_v2, auto-scaling 0-2 instances).

### 4. Run the MCP Server

**Terminal 1 - Start MCP Server:**
```bash
python3 server.py
```

You'll see:
```
INFO:     Uvicorn running on http://127.0.0.1:8000
INFO:     Application startup complete.
```

### 5. Run the Foundry Agent Demo (Interactive)

**Terminal 2 - Start Agent Demo:**
```bash
python3 mcp_foundry_agent.py
```

You'll be prompted to enter your message:
```
ğŸ’¬ Enter your message for the agent (or press Enter for default):
   Default: 'Hi, use the MCP server to run my ML pipeline'
Your message: _
```

**Example prompts:**
- `"Run my ML pipeline"` â†’ Submits pipeline to Azure ML
- `"List all experiments"` â†’ Shows all ML experiments
- `"Check job status"` â†’ Gets status of a specific job
- `"Hello there"` â†’ Calls greet tool

The agent will:
1. âœ… Connect to Azure AI Foundry
2. âœ… Understand your request
3. âœ… Call appropriate MCP tool
4. âœ… Display results

### 6. Test MCP Tools Directly (Optional)

Use the included REST Client test file:

```bash
# Open test-mcp-tools.http in VS Code
# Click "Send Request" above each test
```

Or use curl:
```bash
curl -X POST http://localhost:8000/mcp/call \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "greet", "parameters": {"name": "Azure Community"}}'
```

### 7. Expose MCP Server Publicly (Optional - for Agent Playground)

To use Azure AI Foundry Agent Playground with your local MCP server:

```bash
# Install ngrok
brew install --cask ngrok

# Authenticate (get token from https://dashboard.ngrok.com)
ngrok config add-authtoken YOUR_TOKEN

# Terminal 3 - Expose local server
ngrok http 8000
```

You'll get a public URL like: `https://your-subdomain.ngrok-free.app`

Use this URL in Azure AI Foundry Agent Playground:
```
https://your-subdomain.ngrok-free.app/mcp/call
```

## ğŸ› ï¸ Available MCP Tools

The MCP server exposes the following tools that Foundry Agents can call:

### Core Azure ML Tools (Enterprise Integration)

These are the production tools for real ML operations:

#### `run_aml_pipeline(pipeline_job_yaml, payload, experiment_name)`
Triggers an Azure ML pipeline job.

**Parameters:**
- `pipeline_job_yaml` (str): Path to pipeline YAML (default: "aml/jobs/pipeline.yml")
- `payload` (dict): Data to pass to the pipeline
- `experiment_name` (str): Experiment name (default: "mcp-integration-demo")

**Example:**
```python
run_aml_pipeline(
    pipeline_job_yaml="aml/jobs/pipeline.yml",
    payload={"message": "Process this data", "batch_size": 100},
    experiment_name="my-ml-experiment"
)
```

#### `list_aml_experiments()`
Lists all experiments in the Azure ML workspace.

**Example:**
```python
list_aml_experiments()
# Returns: {"status": "success", "experiments": [...], "count": 5}
```

#### `get_aml_job_status(job_name: str)`
Gets the status and details of a specific Azure ML job.

**Example:**
```python
get_aml_job_status("elastic_mountain_xyz123")
# Returns: {"status": "Running", "job_name": "elastic_mountain_xyz123", ...}
```

### Demo/Testing Tools

These tools demonstrate the MCP workflow and are useful for testing:

#### `greet(name: str)`
Simple greeting function for validating MCP tool calling.

**Example:**
```python
greet("Alice")
# Returns: "Hello, Alice! Welcome to the MCP Foundry ML integration."
```

#### `add_numbers(a: float, b: float)`
Adds two numbers - useful for testing tool parameter passing.

**Example:**
```python
add_numbers(5, 3)
# Returns: {"sum": 8, "inputs": {"a": 5, "b": 3}, "operation": "5 + 3 = 8"}
```

## ğŸ“ Usage Examples

### Example 1: Testing the MCP Server

Test with demo tools to validate the integration:

```bash
# Test via browser
http://localhost:8000/

# Test via API
curl -X POST http://localhost:8000/mcp/call \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "greet", "parameters": {"name": "Azure Community"}}'
```

### Example 2: Trigger ML Pipeline via Agent

Agent understands natural language and triggers ML operations:

Ask the agent to run an ML pipeline:

```
User: "Run the ML pipeline with my training data"

Agent: [calls run_aml_pipeline tool]
Agent: "I've submitted the pipeline job. Job ID: elastic_mountain_xyz123. Status: Running."
```

### Example 3: Check Job Status

```
User: "What's the status of job elastic_mountain_xyz123?"

Agent: [calls get_aml_job_status tool]
Agent: "The job is currently running. It started 5 minutes ago."
```

## ğŸ” Authentication

This project supports multiple Azure authentication methods:

1. **Azure CLI** (Recommended for local development)
   ```bash
   az login
   ```

2. **Managed Identity** (For production deployments)
   - System-assigned identity
   - User-assigned identity

3. **Environment Variables**
   ```bash
   export AZURE_CLIENT_ID=<client-id>
   export AZURE_TENANT_ID=<tenant-id>
   export AZURE_CLIENT_SECRET=<client-secret>
   ```

The `DefaultAzureCredential` from `azure-identity` automatically tries these methods in order.

## ğŸ§ª Development

### Adding New MCP Tools

1. Create the tool function in `src/mcp_server/tools/`:
   ```python
   def my_new_tool(param1: str, param2: int) -> dict:
       """Tool description."""
       # Implementation
       return {"result": "..."}
   ```

2. Register it in `src/mcp_server/main.py`:
   ```python
   @mcp.tool()
   def my_new_tool(param1: str, param2: int) -> dict:
       """Tool description for the agent."""
       return tools.my_new_tool(param1, param2)
   ```

3. Add it to the allowed tools list in `src/foundry_agent/client.py`:
   ```python
   for tool_name in ("greet", "add_numbers", "run_aml_pipeline", "my_new_tool"):
       mcp_tool.allow_tool(tool_name)
   ```

### Testing

Test individual components:

```bash
# Test MCP tools directly
python -c "from src.mcp_server.tools import utility_tools; print(utility_tools.greet('Test'))"

# Test Azure ML bridge
python -c "from src.foundry_agent.bridge import McpAzureMlBridge; bridge = McpAzureMlBridge.from_env(); print(bridge.list_experiments())"
```

## ğŸ“š Key Concepts

### What is MCP (Model Context Protocol)?

MCP is a protocol that creates a stable interface between LLMs and enterprise systems. It makes AI-driven automation:
- **Predictable**: Standardized tool definitions and responses
- **Controlled**: Validation and approval mechanisms
- **Secure**: Proper authentication and authorization

### Why This Architecture?

**Problem**: AI agents (like GPT-4) can't directly call enterprise systems safely.

**Solution**: 
1. **Foundry Agents** provide natural language understanding
2. **MCP Server** acts as a secure gateway with validated tools
3. **Azure ML** handles the actual compute workloads

This separation ensures:
- Security boundaries
- Proper error handling
- Auditability
- Scalability

## ğŸ¤ Contributing

Contributions are welcome! Areas for enhancement:
- Additional Azure ML operations (model deployment, endpoint management)
- Enhanced error handling and retry logic
- Monitoring and logging improvements
- Integration tests
- Documentation improvements

## ğŸ“„ License

This project is provided as-is for educational and demonstration purposes.

## ğŸ“– Documentation

- **[Implementation Guide](docs/IMPLEMENTATION_GUIDE.md)** - Detailed step-by-step walkthrough of how this was built
- **[Architecture Diagram](docs/diagrams/mcp-foundry-azureml-integration.jpeg)** - Visual representation of the system

## ğŸ¤ Presentation

This project was presented at the **Azure NG Community Tech Call - November 2025**

**Key Talking Points:**
1. The problem: AI agents can't safely call enterprise systems directly
2. The solution: MCP as a secure middleware layer
3. The implementation: 3-layer architecture (Intelligence, Integration, Execution)
4. The benefits: Security, governance, standardization, scalability

## ğŸ”— Resources

- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)
- [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-services/ai-foundry/)
- [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/)
- [Microsoft Foundry Agents SDK](https://learn.microsoft.com/en-us/azure/ai-services/agents/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

## ğŸ†˜ Troubleshooting

### Common Issues

**Issue**: `ModuleNotFoundError: No module named 'mcp_server'`
- **Solution**: Ensure you're running from the project root and `src/` is in `sys.path`

**Issue**: `KeyError: 'AZURE_SUBSCRIPTION_ID'`
- **Solution**: Configure your `.env` file or set environment variables

**Issue**: `Authentication failed`
- **Solution**: Run `az login` and ensure you have access to the resources

**Issue**: Pipeline job fails with "compute not found"
- **Solution**: Update `aml/jobs/pipeline.yml` with your actual compute cluster name

---

## ğŸ“ Key Files Explained

| File/Folder | Purpose |
|-------------|---------|
| **server.py** | MCP Server entry point - Starts FastAPI server on port 8000 |
| **mcp_foundry_agent.py** | Interactive agent demo - Prompts for user input and demonstrates full workflow |
| **test-mcp-tools.http** | REST Client tests - Quick testing of all 5 MCP tools |
| **.env** | **Your credentials** - Never commit this! (Already in .gitignore) |
| **.env.example** | Template for .env - Safe to commit, no secrets |
| **aml/jobs/pipeline.yml** | Azure ML pipeline definition - Simple demo pipeline |
| **aml/models/** | Reserved for model registrations (currently unused, for future expansion) |
| **src/mcp_server/main.py** | FastAPI server with tool routing logic |
| **src/mcp_server/tools/** | Tool implementations (utility_tools.py, azure_ml_tools.py) |
| **src/foundry_agent/client.py** | Agent demo with interactive prompts and keyword-based tool selection |
| **src/foundry_agent/bridge.py** | Azure ML operations (submit pipeline, list experiments, get job status) |
| **scripts/setup.sh** | Automated setup - Checks prereqs, creates venv, installs deps, guides config |
| **scripts/create-compute.sh** | Creates Azure ML compute cluster (mcp-compute) |
| **docs/IMPLEMENTATION_GUIDE.md** | Detailed step-by-step implementation walkthrough |
| **docs/PRESENTATION_GUIDE.md** | Quick reference for tech talk presentation |

## ğŸ”’ Security Notes

âœ… **All credentials are stored in `.env` file only**
âœ… **`.env` is in `.gitignore` - never committed to git**
âœ… **No hardcoded secrets in any source code**
âœ… **`.env.example` is safe to share - contains only placeholders**

Always use `.env.example` as a template when sharing this project.

---

**Built with â¤ï¸ for the Azure NG Community Tech Talk - November 2025**

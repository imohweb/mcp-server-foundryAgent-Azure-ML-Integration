# Implementation Guide: MCP Foundry ML Integration

## ðŸ“‹ Overview

This guide provides a **step-by-step walkthrough** of how the MCP Foundry ML integration was implemented, connecting Microsoft Foundry Agents to Azure Machine Learning through the Model Context Protocol (MCP).

---

## ðŸŽ¯ Implementation Goals

1. Create a secure bridge between AI agents and enterprise ML systems
2. Expose Azure ML operations as standardized MCP tools
3. Enable natural language interaction with ML workflows
4. Maintain security, validation, and governance

---

## ðŸ—ï¸ Architecture Layers

### **Layer 1: Intelligence (Foundry Agent)**
- **Purpose**: Natural language understanding and decision-making
- **Technology**: Microsoft Foundry Agents, GPT-4o
- **Responsibility**: Interprets user intent and calls appropriate tools

### **Layer 2: Integration (MCP Server)**
- **Purpose**: Secure middleware for tool orchestration
- **Technology**: FastAPI, Python, MCP Protocol
- **Responsibility**: Exposes validated tools and routes to enterprise systems

### **Layer 3: Execution (Azure ML)**
- **Purpose**: Enterprise ML operations
- **Technology**: Azure Machine Learning SDK
- **Responsibility**: Executes pipelines, manages experiments, tracks jobs

---

## ðŸ“ Step-by-Step Implementation

### **Step 1: Project Structure Setup**

First, we created a clear project structure separating concerns:

```
mcp-foundry-ml/
â”œâ”€â”€ server.py                    # MCP Server entry point
â”œâ”€â”€ mcp_foundry_agent.py        # Foundry Agent entry point
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ .env.example                # Environment template
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp_server/             # Layer 2: MCP Server
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI server with tool endpoints
â”‚   â”‚   â”œâ”€â”€ config.py           # Server configuration
â”‚   â”‚   â””â”€â”€ tools/              # Tool implementations
â”‚   â”‚       â”œâ”€â”€ utility_tools.py    # Demo tools (greet, add_numbers)
â”‚   â”‚       â””â”€â”€ azure_ml_tools.py   # Azure ML integrations
â”‚   â”‚
â”‚   â””â”€â”€ foundry_agent/          # Layer 1: Foundry Agent
â”‚       â”œâ”€â”€ client.py           # Agent implementation
â”‚       â””â”€â”€ bridge.py           # Azure ML connection bridge
â”‚
â””â”€â”€ aml/
    â””â”€â”€ jobs/
        â””â”€â”€ pipeline.yml        # Azure ML pipeline definition
```

**Key Decision**: Separate MCP server logic from Azure ML logic for maintainability.

---

### **Step 2: Define Dependencies**

Created `requirements.txt` with essential packages:

```python
# MCP Server Dependencies
mcp>=1.0.0
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0

# Azure Dependencies
azure-ai-ml>=1.13.0
azure-identity>=1.15.0
azure-ai-projects>=1.0.0b1

# Utilities
pyyaml>=6.0.1
python-dotenv>=1.0.0
```

**Key Decision**: Use FastAPI for HTTP/REST endpoints to make the MCP server web-accessible.

---

### **Step 3: Implement Azure ML Bridge**

Created `src/foundry_agent/bridge.py` to handle Azure ML operations:

```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

class McpAzureMlBridge:
    def __init__(self, ml_client: MLClient):
        self.ml_client = ml_client
    
    @classmethod
    def from_env(cls):
        """Create bridge from environment variables"""
        credential = DefaultAzureCredential()
        ml_client = MLClient(
            credential=credential,
            subscription_id=os.environ["AZURE_SUBSCRIPTION_ID"],
            resource_group_name=os.environ["AZURE_RESOURCE_GROUP"],
            workspace_name=os.environ["AZURE_ML_WORKSPACE"],
        )
        return cls(ml_client)
    
    def run_pipeline_direct(self, pipeline_job_yaml, payload, experiment_name):
        """Submit pipeline job to Azure ML"""
        job = load_job(pipeline_job_yaml)
        job.inputs["input_data"] = payload
        job.experiment_name = experiment_name
        
        submitted = self.ml_client.jobs.create_or_update(job)
        return {
            "job_name": submitted.name,
            "job_id": submitted.id,
            "status": submitted.status,
        }
```

**Key Decision**: Use `DefaultAzureCredential` for flexible authentication (CLI, managed identity, service principal).

---

### **Step 4: Create Tool Implementations**

#### **4.1 Utility Tools** (`src/mcp_server/tools/utility_tools.py`)

Simple demo tools to test the MCP workflow:

```python
def greet(name: str) -> str:
    """Generate a friendly greeting message."""
    return f"Hello, {name}! Welcome to the MCP Foundry ML integration."

def add_numbers(a: float, b: float) -> Dict[str, Any]:
    """Add two numbers and return the result with metadata."""
    result = a + b
    return {
        "sum": result,
        "inputs": {"a": a, "b": b},
        "operation": f"{a} + {b} = {result}"
    }
```

**Purpose**: Validate MCP tool calling before adding complex Azure ML operations.

#### **4.2 Azure ML Tools** (`src/mcp_server/tools/azure_ml_tools.py`)

Enterprise tools for real ML operations:

```python
def run_aml_pipeline(pipeline_job_yaml, payload, experiment_name):
    """Trigger an Azure ML pipeline job"""
    bridge = McpAzureMlBridge.from_env()
    job_info = bridge.run_pipeline_direct(
        pipeline_job_yaml, payload, experiment_name
    )
    return {
        "status": "submitted",
        "job": job_info,
        "message": f"Pipeline job submitted successfully"
    }

def list_experiments():
    """List all experiments in the workspace"""
    bridge = McpAzureMlBridge.from_env()
    experiments = bridge.list_experiments()
    return {
        "status": "success",
        "experiments": experiments,
        "count": len(experiments)
    }

def get_job_status(job_name: str):
    """Get the status of a specific job"""
    bridge = McpAzureMlBridge.from_env()
    job_details = bridge.get_job_details(job_name)
    return job_details
```

**Key Decision**: Wrap all Azure operations with try-except for robust error handling.

---

### **Step 5: Build the MCP Server**

Created `src/mcp_server/main.py` using FastAPI for HTTP access:

```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="MCP Foundry Bridge Server")

class ToolRequest(BaseModel):
    tool_name: str
    parameters: Dict[str, Any]

@app.get("/")
async def root():
    """Server info page"""
    return HTMLResponse(content="""
        <h1>MCP Foundry Bridge Server</h1>
        <p>Available tools: greet, add_numbers, run_aml_pipeline, etc.</p>
    """)

@app.get("/tools")
async def list_tools():
    """List all available tools"""
    return {
        "tools": [
            {"name": "greet", "description": "Greet someone"},
            {"name": "run_aml_pipeline", "description": "Trigger ML pipeline"},
            # ... more tools
        ]
    }

@app.post("/mcp/call")
async def call_tool(request: ToolRequest):
    """Execute a tool"""
    if request.tool_name == "greet":
        result = greet(**request.parameters)
    elif request.tool_name == "run_aml_pipeline":
        result = run_aml_pipeline(**request.parameters)
    # ... route to other tools
    
    return {"status": "success", "result": result}

def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Key Decision**: Use FastAPI for:
- Web browser accessibility
- Interactive API docs (`/docs`)
- RESTful tool calling
- Easy testing and debugging

---

### **Step 6: Configure Environment Variables**

Created `.env.example` as a template:

```bash
# Azure ML Configuration
AZURE_SUBSCRIPTION_ID=00000000-0000-0000-0000-000000000000
AZURE_RESOURCE_GROUP=my-resource-group
AZURE_ML_WORKSPACE=my-aml-workspace

# Foundry Agent Configuration
PROJECT_ENDPOINT=https://my-project.eastus.api.azureml.ms
MODEL_DEPLOYMENT_NAME=gpt-4o

# MCP Server Configuration
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000
MCP_SERVER_URL=http://localhost:8000/mcp
```

**Key Decision**: Use environment variables for secrets and configuration (12-factor app methodology).

---

### **Step 7: Create Azure ML Pipeline Definition**

Defined a simple pipeline in `aml/jobs/pipeline.yml`:

```yaml
$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline
description: Simple MCP â†’ Azure ML pipeline

inputs:
  input_data:
    type: any

jobs:
  echo_step:
    type: command
    command: >-
      python -c "import json; print('Received:', json.dumps(${{inputs.input_data}}))"
    environment:
      image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04
    compute: azureml:cpu-cluster
    inputs:
      input_data: ${{parent.inputs.input_data}}
```

**Key Decision**: Keep the first pipeline simple to validate the integration end-to-end.

---

### **Step 8: Implement Foundry Agent Client**

Created `src/foundry_agent/client.py` (template for SDK integration):

```python
from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential

def run_foundry_agent_demo():
    """
    Connect to Foundry project and create an agent with MCP tools
    """
    project_endpoint = os.environ["PROJECT_ENDPOINT"]
    model_deployment = os.environ["MODEL_DEPLOYMENT_NAME"]
    
    credential = DefaultAzureCredential()
    
    # TODO: Implement with azure-ai-projects SDK
    # 1. Create agent with MCP tool definitions
    # 2. Send user message
    # 3. Handle tool calls
    # 4. Return responses
```

**Note**: Full implementation depends on the specific `azure-ai-projects` SDK version and MCP integration APIs.

---

### **Step 9: Create Entry Points**

#### **9.1 MCP Server Entry** (`server.py`)
```python
import sys
from pathlib import Path

ROOT_DIR = Path(__file__).parent
SRC_DIR = ROOT_DIR / "src"
sys.path.append(str(SRC_DIR))

from mcp_server.main import run_server

if __name__ == "__main__":
    run_server()
```

#### **9.2 Foundry Agent Entry** (`mcp_foundry_agent.py`)
```python
import sys
from pathlib import Path

ROOT_DIR = Path(__file__).parent
SRC_DIR = ROOT_DIR / "src"
sys.path.append(str(SRC_DIR))

from foundry_agent.client import run_foundry_agent_demo

if __name__ == "__main__":
    run_foundry_agent_demo()
```

---

### **Step 10: Setup and Testing**

#### **10.1 Create Virtual Environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### **10.2 Install Dependencies**
```bash
pip install -r requirements.txt
```

#### **10.3 Configure Environment**
```bash
cp .env.example .env
# Edit .env with your actual Azure credentials
```

#### **10.4 Authenticate to Azure**
```bash
az login
az account set --subscription <your-subscription-id>
```

#### **10.5 Start MCP Server**
```bash
python server.py
```

Server runs on: `http://localhost:8000/`

#### **10.6 Test the Server**

**Browser Test:**
- Navigate to: `http://localhost:8000/`
- View tools: `http://localhost:8000/tools`
- API docs: `http://localhost:8000/docs`

**Command Line Test:**
```bash
# Test greet tool
curl -X POST http://localhost:8000/mcp/call \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "greet", "parameters": {"name": "Azure"}}'

# Test add_numbers tool
curl -X POST http://localhost:8000/mcp/call \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "add_numbers", "parameters": {"a": 10, "b": 32}}'
```

---

## ðŸ”‘ Key Implementation Decisions

### **1. Why FastAPI Instead of Pure MCP?**
- **HTTP Accessibility**: Can be accessed from browsers and any HTTP client
- **Interactive Docs**: FastAPI auto-generates `/docs` for testing
- **REST Standards**: Familiar patterns for enterprise integration
- **Debugging**: Easier to inspect and test with curl/Postman

### **2. Why Separate Bridge Class?**
- **Separation of Concerns**: MCP server doesn't need to know Azure ML details
- **Reusability**: Bridge can be used by other components
- **Testability**: Can mock the bridge for unit tests
- **Maintainability**: Azure ML changes don't affect MCP server logic

### **3. Why Environment Variables?**
- **Security**: Keeps credentials out of code
- **Flexibility**: Easy to change between dev/test/prod
- **12-Factor App**: Industry best practice for cloud apps
- **CI/CD Friendly**: Easy to inject in deployment pipelines

### **4. Why Demo Tools (greet, add_numbers)?**
- **Validation**: Test MCP workflow without Azure dependencies
- **Learning**: Help understand the tool-calling pattern
- **Debugging**: Quick feedback loop during development
- **Demo**: Easy to show functionality in presentations

---

## ðŸ”„ Request Flow Example

Let's trace a complete request from user to Azure ML:

### **User Request**: "Run my ML pipeline with batch size 100"

#### **1. Foundry Agent (Layer 1)**
```
User: "Run my ML pipeline with batch size 100"
  â†“
Agent processes natural language
  â†“
Agent decides to call: run_aml_pipeline
  â†“
Agent prepares parameters: {
  "pipeline_job_yaml": "aml/jobs/pipeline.yml",
  "payload": {"batch_size": 100},
  "experiment_name": "user-experiment"
}
```

#### **2. MCP Server (Layer 2)**
```
Receives HTTP POST to /mcp/call
  â†“
Validates request schema
  â†“
Routes to run_aml_pipeline function
  â†“
Calls azure_ml_tools.run_aml_pipeline(**params)
```

#### **3. Azure ML Bridge**
```
Creates McpAzureMlBridge instance
  â†“
Authenticates with DefaultAzureCredential
  â†“
Loads pipeline YAML definition
  â†“
Injects payload into pipeline inputs
  â†“
Calls ml_client.jobs.create_or_update()
```

#### **4. Azure ML (Layer 3)**
```
Receives pipeline job submission
  â†“
Validates pipeline definition
  â†“
Allocates compute resources
  â†“
Starts pipeline execution
  â†“
Returns job metadata
```

#### **5. Response Flow (Back to User)**
```
Azure ML â†’ Bridge: {job_name, job_id, status}
  â†“
Bridge â†’ MCP Server: {status: "submitted", job: {...}}
  â†“
MCP Server â†’ Agent: HTTP 200 with job details
  â†“
Agent â†’ User: "I've submitted your ML pipeline. Job ID: xyz123. Status: Running"
```

---

## ðŸ› ï¸ Troubleshooting Common Issues

### **Issue 1: Module Not Found Errors**
```
ModuleNotFoundError: No module named 'mcp_server'
```
**Solution**: Ensure you're running from project root and `src/` is in `sys.path`

### **Issue 2: Authentication Failed**
```
DefaultAzureCredential failed to retrieve a token
```
**Solution**: Run `az login` and verify subscription access

### **Issue 3: Port Already in Use**
```
ERROR: [Errno 48] Address already in use
```
**Solution**: Kill existing process or change port in config

### **Issue 4: Pipeline Job Fails**
```
ComputeNotFound: Compute cluster 'cpu-cluster' not found
```
**Solution**: Update `aml/jobs/pipeline.yml` with your actual compute name

---

## ðŸ“Š Testing Strategy

### **Unit Tests** (Individual Functions)
```python
# Test utility tools
assert greet("Azure") == "Hello, Azure! Welcome to..."
assert add_numbers(5, 3)["sum"] == 8

# Test bridge (with mocked ML client)
mock_client = Mock()
bridge = McpAzureMlBridge(mock_client)
result = bridge.run_pipeline_direct(...)
assert result["status"] == "submitted"
```

### **Integration Tests** (MCP Server)
```python
from fastapi.testclient import TestClient
from mcp_server.main import app

client = TestClient(app)

def test_list_tools():
    response = client.get("/tools")
    assert response.status_code == 200
    assert len(response.json()["tools"]) > 0

def test_call_tool():
    response = client.post("/mcp/call", json={
        "tool_name": "greet",
        "parameters": {"name": "Test"}
    })
    assert response.status_code == 200
    assert "Hello" in response.json()["result"]
```

### **End-to-End Tests** (Full Stack)
```bash
# 1. Start MCP server
python server.py &

# 2. Send test request
curl -X POST http://localhost:8000/mcp/call \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "run_aml_pipeline", "parameters": {...}}'

# 3. Verify in Azure ML
az ml job show --name <job-name> --resource-group <rg> --workspace-name <ws>
```

---

## ðŸš€ Deployment Considerations

### **Production Checklist**

- [ ] Remove or secure demo tools (greet, add_numbers)
- [ ] Add authentication/authorization to MCP server
- [ ] Implement rate limiting
- [ ] Add comprehensive logging and monitoring
- [ ] Set up Application Insights for telemetry
- [ ] Use Azure Key Vault for secrets
- [ ] Deploy to Azure Container Apps or AKS
- [ ] Configure HTTPS/TLS
- [ ] Set up CI/CD pipeline
- [ ] Document SLA and support procedures

### **Scaling Options**

1. **Horizontal Scaling**: Deploy multiple MCP server instances behind load balancer
2. **Azure Container Apps**: Auto-scaling based on HTTP requests
3. **Azure Kubernetes Service**: Full orchestration for complex scenarios
4. **Azure Functions**: Serverless option for sporadic usage

---

## ðŸ“š Additional Resources

- [Model Context Protocol Specification](https://modelcontextprotocol.io/)
- [Azure Machine Learning Documentation](https://learn.microsoft.com/azure/machine-learning/)
- [Microsoft Foundry Agents](https://learn.microsoft.com/azure/ai-services/agents/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Azure Identity SDK](https://learn.microsoft.com/python/api/azure-identity/)

---

## âœ… Summary

This implementation successfully creates a **3-layer architecture** that:

1. âœ… Enables natural language interaction with ML workflows
2. âœ… Provides secure, validated tool calling via MCP
3. âœ… Integrates seamlessly with Azure Machine Learning
4. âœ… Maintains enterprise governance and control
5. âœ… Supports web-based testing and debugging

**Result**: AI agents can now safely trigger enterprise ML workflows through a standardized, secure interface! ðŸŽ‰

---

**Built for the Azure NG Community Tech Talk - November 2025**
